#+TITLE: Markov Chain Monte Carlo 
#+DATE: \today
#+HUGO_BASE_DIR: /home/eherbst/Dropbox/www/
#+HUGO_SECTION: teaching/bank-of-colombia-smc/lectures
#+hugo_custom_front_matter: :math true
#+hugo_auto_set_lastmod: t
#+MACRO: NEWLINE @@latex:\\~\\~@@ @@html:<br>@@ @@ascii:|@@
#+OPTIONS: toc:nil H:2
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{helvet}
#+LaTEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \bibliographystyle{ecta}
#+LaTEX_HEADER: \beamertemplatenavigationsymbolsempty
#+LaTeX_HEADER: \usepackage{bibentry}
#+LaTeX_HEADER: \nobibliography*
#+LaTeX_HEADER: \makeatletter\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}\makeatother
#+LaTeX_HEADER: \newtheorem{algo}{Algorithm}
#+LaTeX_CLASS: beamer

* Metropolis-Hastings Algorithm

** The Metropolis-Hastings Algorithm


- Metropolis-Hastings (MH) algorithm belongs to the class of Markov chain
  Monte Carlo (MCMC) algorithms.
  {{{NEWLINE}}}
- Algorithm constructs a Markov chain such that the stationary distribution
  associated with this Markov chain is unique and equals the posterior
  distribution of interest.
  {{{NEWLINE}}}
- First version constructed by cite:Metropolis1953. Later
  generalized by cite:Hastings1970.  cite:Tierney1994 proved
  important convergence results for MCMC algorithms.
  {{{NEWLINE}}}
- Introduction: cite:Chib1995a. Textbook cite:Robert2004 or
  cite:Geweke2005.



** Markov Chain Monte Carlo

- Importance sampler generates a sequence of independent draws
  from the posterior distribution $\pi(\theta)$, the MH algorithm generates
  a sequence of serially correlated draws.
  {{{NEWLINE}}}
- As long as the correlation in the Markov chain is not too strong, Monte
  Carlo averages \index{Monte Carlo average} of these draws can accurately
  approximate posterior means of $h(\theta)$.
  {{{NEWLINE}}} 
- We are going to care a lot about this correlation.  Why? 
  \[
    \sqrt{n}( \bar{X} - \mathbb{E}[\bar{X}]) \Longrightarrow N \bigg( 0, \, \frac{1}{n} \sum_{i=1}^n \mathbb{V}[X_i]
                  + {\color{red} \frac{1}{n} \sum_{i=1}^n \sum_{j \not=i} COV(X_i,X_j)} \bigg).
  \]



** The Metropolis Hastings Algorithm
A key ingredient is the proposal distribution \index{proposal density}
$q(\vartheta|\theta^{i-1})$, which potentially depends on the draw
$\theta^{i-1}$ in iteration $i-1$ of the algorithm.
\vspace{0.05in}
\begin{algo}[Generic MH Algorithm]
	\label{algo_genericmh}
	For $i=1$ to N:
	Draw $\vartheta$ from a density $q(\vartheta|\theta^{i-1})$.
	Set $\theta^{i} = \vartheta$ with probability
	   \[
	   \alpha(\vartheta | \theta^{i-1} ) = \min \left\{ 1, \;
	   \frac{ p(Y| \vartheta )p(\vartheta) / q(\vartheta | \theta^{i-1}) }{
	           p(Y|\theta^{i-1}) p(\theta^{i-1})  / q(\theta^{i-1} | \vartheta) } \right\}
	   \]
	   and $\theta^{i} = \theta^{i-1}$ otherwise.
\end{algo}
{{{NEWLINE}}}
Because $p(\theta|Y) \propto p(Y|\theta)p(\theta)$ we can replace the posterior
densities in the calculation of the acceptance probabilities $\alpha(\vartheta |
\theta^{i-1})$
This yields a Markov transition kernel $K(\theta|\tilde{\theta})$, where
the conditioning value $\tilde{\theta}$ corresponds to the parameter draw from
iteration $i-1$.


** Convergence
  Probability theory for MH is much harder than for IS.
  - Suppose that $\theta^0\sim g(\cdot)$ and $\theta^N$ is obtained by
    iterating the Markov transition kernel forward $N$ times, then is it
    true that $\theta^N$ is approximately distributed according to
    $p(\theta|Y)$ and the approximation error vanishes as $N \longrightarrow
    \infty$?
  - Suppose that (i) is true, is it also true that sample averages of
    $\theta^i$, $i=1,\ldots,N$ satisfy a SLLN and a CLT?


  Key property: *invariance* of Markov Chain. 
  \begin{eqnarray}
    p(\theta|Y) = \int K(\theta | \tilde{\theta}) p(\tilde{\theta} |Y) d \tilde{\theta}.
    \label{eq_mhinvariance}
  \end{eqnarray}
  Show this property using {\bf reversibility of the Markov Chain}
  {{{NEWLINE}}}
  Not sufficient for SLLN or CLT, these things depend on $q$ and $\pi$. 
  {{{NEWLINE}}}
  \textcolor{red}{Look at specific example.}


** A Specific Example
  
    - Suppose the parameter space is discrete and $\theta$ can only take two
      values: $\tau_1$ and $\tau_2$.
    - The posterior distribution then simplifies to two probabilities which
      we denote as $\pi_l = \mathbb{P}\{ \theta=\tau_l|Y\}$, $l=1,2$.
    - The proposal distribution in Algorithm~\ref{algo_genericmh}
      can be represented as a two-stage Markov process with transition
      matrix
      \begin{equation}
      Q = \left[ \begin{array}{cc} q_{11} & q_{12} \\ q_{21} & q_{22} \end{array} \right],
      \end{equation}
      where $q_{lk}$ is the probability of drawing $\vartheta = \tau_k$ conditional
      on $\theta^{i-1} = \tau_l$.
    - Assume that
      \[
      q_{11} = q_{22} = q, \quad q_{12}=q_{21}=1-q
      \]
      and that the posterior distribution has the property
      \[
      \pi_2 > \pi_1.
      \]
    


** Deriving the Transition Kernel
  - Suppose that $\theta^{i-1} = \tau_1$. Then with probability $q$,
    $\vartheta=\tau_1$. The probability that this draw will be
    accepted is
    \[
    \alpha(\tau_1|\tau_1) = \min \left\{ 1, \frac{ \pi_1/ q}{\pi_1 / q} \right\} = 1.
    \]
  - With probability $1-q$ the proposed draw is $\vartheta = \tau_2$. The probability
    that this draw will be rejected is
    \[
    1-\alpha(\tau_2|\tau_1) = 1-\min \left\{ 1, \frac{ \pi_2/ (1-q)}{\pi_1 / (1-q)} \right\} = 0
    \]
    because we previously assumed that $\pi_2 > \pi_1$.
  - The probability of a transition from $\theta^{i-1}=\tau_1$ to $\theta^{i}=\tau_1$ is
    \[
    k_{11} = q \cdot 1 + (1-q) \cdot 0 = q.
    \]



** Transition Kernel, Continued

- Similar reasoning as before
  \begin{eqnarray}
  K = \left[ \begin{array}{cc} k_{11} & k_{12} \\ k_{21} & k_{22} \end{array} \right] \label{eq_exKtransition}
    = \left[ \begin{array}{cc} q & (1-q) \\ (1-q)\frac{\pi_1}{\pi_2} &  q + (1-q)\left( 1- \frac{\pi_1}{\pi_2} \right) \end{array} \right]. \nonumber
  \end{eqnarray}
  {{{NEWLINE}}}
- $K$ has two eigenvalues $\lambda_1$ and $\lambda_2$:
  \begin{equation}
  \lambda_1(K) = 1, \quad \lambda_2(K) = q - (1-q)\frac{\pi_1}{1-\pi_1}.
  \end{equation}
  Eigenvector associated with with $\lambda_1(K)$ determines the invariant distribution
  of the Markov chain (=posterior).  If $\lambda_2(K)\neq 1$, this distribution is unique.
  {{{NEWLINE}}}

  The persistence of the Markov chain is characterized by the eigenvalue $\lambda_2(K)$.



** Markov Chain
  
  We can represent the Markov Chain generated by MH as an AR(1).  Define:
  \[
  \xi^i = \frac{\theta^i - \tau_1}{\tau_2-\tau_1},\quad \xi^i \in \{0, 1\}.
  \]
  $\xi^i$ follows the first-order autoregressive process
  \begin{equation}
  \xi^i = (1-k_{11}) + \lambda_2(K) \xi^{i-1} + \nu^i.
  \label{eq_exKxi}
  \end{equation}
  {{{NEWLINE}}}
  Conditional on $\xi^{i-1}=j-1$, $j=1,2$, the innovation $\nu^i$ has
  support on $k_{jj}$ and $(1-k_{jj})$, its conditional mean is equal to
  zero, and its conditional variance is equal to $k_{jj}(1-k_{jj})$.

    


** More on Markov Chain
  
  - Persistence of the Markov chain depends on the proposal distribution,
    which in our discrete example is characterized by the probability $q$.
    {{{NEWLINE}}}
  - You could get an $iid$ sample from the posterior by setting $q =\pi_1$, so
    $\lambda_2(K)=0$.)
    {{{NEWLINE}}}
  - OTOH, if $q=1$, then $\theta^i=\theta^1$ for all $i$ and the equilibrium
    distribution of the chain is no longer unique.
    {{{NEWLINE}}}
  - General goal of MCMC: keep the persistence of the chain as low as possible.
  


** 
\[
\bar{h}_N = \frac{1}{N} \sum_{i=1}^N h(\theta^i)
\]
we deduce from a central limit theorem for dependent random variables that
\[
\sqrt{N} (\bar{h}_N - \mathbb{E}_\pi[h])
\Longrightarrow N \big(0, \Omega(h) \big),
\]
where $\Omega(h)$ is now the long-run covariance matrix
\[
\Omega(h) = \lim_{L \longrightarrow \infty} \mathbb{V}_\pi[h] \left( 1 + 2 \sum_{l=1}^L \frac{L-l}{L} \left( q - (1-q)\frac{\pi_1}{1-\pi_1} \right)^l \right).
\]
In turn, the asymptotic inefficiency factor is given by \index{inefficiency factor}
\begin{eqnarray}
\mbox{InEff}_\infty &=& \frac{\Omega(h)}{\mathbb{V}_\pi[h]} \\
&=& 1 + 2 \lim_{L \longrightarrow \infty} \; \sum_{l=1}^L \frac{L-l}{L} \left( q - (1-q)\frac{\pi_1}{1-\pi_1} \right)^l. \nonumber
\end{eqnarray}


** Numerical Example

  
  - Bernoulli distribution ($\tau_1 = 0, \tau_2 = 1$) with $\pi_1 = 0.2$.
    {{{NEWLINE}}}
  - Assess the effectiveness of different MH settings, we vary $q \in [0, 1)$.
    {{{NEWLINE}}}
  - Look at autocorrelation for $q= \{0, 0.2, 0.5, 0.99\}$. 
    {{{NEWLINE}}}
  - $\mbox{Ineff}_\infty$ for $q \in [0, 1)$.
    {{{NEWLINE}}}
  - Relationship between across chain variance and within chain (HAC)
    estimates.  This the heart of many convergence statistics.
  


** Autocorrelation Functions

  \includegraphics[width=4.3in]{static/mh_acf}


** Log Inefficiency Factor as function of $q$

\includegraphics[width=4.3in]{static/mh_relative_variance}


** Convergence: within vs across chain variance estimates
  \begin{center}
    \includegraphics[width=2.1in]{static/mh_hac}
  \end{center}


** Take Aways
  
  - high autocorrelation reflects the fact that it will take a high number
    of draws to accurately reflect the \index{target distribution} target
    distribution

    {{{NEWLINE}}}
  - for large values of $q$, the variance of Monte Carlo estimates of $h$
    drawn from the MH chain are much larger than the variance of estimates
    derived from $iid$ draws

    {{{NEWLINE}}}
  - HAC estimates bracket small-sample estimates, indicating convergence,
    but they tend to underestimate variance for all $q$.
  
  
  *How to pick $q$ for a DSGE model?*




** Random Walk Metropolis-Hastings

  - Most popular $q$ for DSGE Models.  
    {{{NEWLINE}}}
  - $q(\vartheta|\theta^{i-1})$ can be expressed as the random walk $\vartheta = \theta^{i-1} + \eta$
    {{{NEWLINE}}}
  - $\eta$ is normally distributed with mean zero and variance $c^2\hat{\Sigma}$.
    {{{NEWLINE}}}
  - Given the symmetric
    nature of the proposal distribution, the acceptance probability \index{acceptance probability} becomes
    \[
    \alpha = \min\left\{\frac{p(\vartheta|Y)}{p(\theta^{i-1}|Y)}, 1 \right\}.
    \]
  - Still need to specify $c$ and $\hat \Sigma$. 



** On $\hat\Sigma$

  - Want $\hat\Sigma$ to incorporate information about the posterior.

  - One approach: \cite{Schorfheide2000}, is to set $\hat\Sigma$ to be the
    negative of the inverse \index{Hessian matrix} Hessian at the mode of the
    log posterior, $\hat\theta$, obtained by running a numerical optimization
    \index{numerical optimization}. 
    {{{NEWLINE}}}
    This has appealing large sample properties, but can be tedious and innacurate. 

  - Another (adaptive) approach: use prior variance for a first sequence of
    posterior draws, the compute the sample covariance matrix and use that as
    $\hat\Sigma$.  /Must be fixed eventually/.

  - Here we cheat:
    \[
    \mbox{RWMH-V} : \hat\Sigma =\mathbb{V}_\pi[\theta].
    \]



** Picking Scaling $c$
  - Goldilocks principal: choose $c$ so that you don't reject too much or too little. 
    {{{NEWLINE}}}
  - \cite{RobertsEtAl1997} have derived a limit (in the size of parameter
    vector) optimal acceptance rate of $0.234$ for a special case (normal
    posterior).
    {{{NEWLINE}}}
  -  Most practitioners target an acceptance rate between
    $0.20$ and $0.40$.
    {{{NEWLINE}}}
  - Requites pre-estimation tuning. 
  


** Baseline Estimation
  \begin{table}[t!]
	\caption{Posterior Estimates of DSGE Model Parameters}
	\label{t_dsge1_posterior}
	\begin{center}
		\begin{tabular}{p{0.8cm}ccp{0.8cm}cc} \hline\hline
			 & Mean & [0.05, 0.95] &  & Mean & [0.05,0.95] \\ \hline
			$\tau$               &  2.83 & [ 1.95,  3.82]  & $\rho_r$             &  0.77 & [ 0.71,  0.82] \\
			$\kappa$             &  0.78 & [ 0.51,  0.98]  & $\rho_g$             &  0.98 & [ 0.96,  1.00] \\
			$\psi_1$             &  1.80 & [ 1.43,  2.20]  & $\rho_z$             &  0.88 & [ 0.84,  0.92] \\
			$\psi_2$             &  0.63 & [ 0.23,  1.21]  & $\sigma_r$           &  0.22 & [ 0.18,  0.26] \\
			$r^{(A)}$            &  0.42 & [ 0.04,  0.95]  & $\sigma_g$           &  0.71 & [ 0.61,  0.84] \\
			$\pi^{(A)}$          &  3.30 & [ 2.78,  3.80]  & $\sigma_z$           &  0.31 & [ 0.26,  0.36] \\
			$\gamma^{(Q)}$       &  0.52 & [ 0.28,  0.74]  &  & & \\
			\hline \hline
		\end{tabular}
	\end{center}
	{\em Notes:} We generated $N=100,000$ draws from the posterior and discarded the first 50,000 draws.
	Based on the remaining draws we approximated the posterior mean and the 5th and 95th percentiles.
\end{table}

  
** More on $c$
  Vary $c\in(0, 2]$.  Look at effect on 
  
    - Acceptance Rate
    - $Ineff_{\infty}$
    - $Ineff_{N}$
    
    What is the relationship between acceptance rate and accuracy?

** Effects of Scaling
   \includegraphics[width=4.3in]{static/dsge1_rwmh_scaling}


** Acceptance Rate vs. Accuracy
\includegraphics[width=4.3in]{static/dsge1_rwmh_acceptance_v_accuracy}


** References
   
   [[bibliography:../../../ref/ref.bib]]
